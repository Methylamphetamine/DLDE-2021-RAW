{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mblade.seas.upenn.edu\u001b[m  Mon Oct 18 20:21:07 2021  \u001b[1m\u001b[30m455.45.01\u001b[m\r\n",
      "\u001b[36m[0]\u001b[m \u001b[34mRTX A6000       \u001b[m |\u001b[1m\u001b[31m 85'C\u001b[m, \u001b[1m\u001b[32m100 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m44688\u001b[m / \u001b[33m48685\u001b[m MB | \u001b[1m\u001b[30msifanw\u001b[m(\u001b[33m44685M\u001b[m)\r\n",
      "\u001b[36m[1]\u001b[m \u001b[34mRTX A6000       \u001b[m |\u001b[31m 46'C\u001b[m, \u001b[1m\u001b[32m 53 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m46182\u001b[m / \u001b[33m48685\u001b[m MB | \u001b[1m\u001b[30mgkissas\u001b[m(\u001b[33m46173M\u001b[m)\r\n",
      "\u001b[36m[2]\u001b[m \u001b[34mRTX A6000       \u001b[m |\u001b[1m\u001b[31m 86'C\u001b[m, \u001b[1m\u001b[32m 98 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m44770\u001b[m / \u001b[33m48685\u001b[m MB | \u001b[1m\u001b[30mshyamss\u001b[m(\u001b[33m44767M\u001b[m)\r\n",
      "\u001b[36m[3]\u001b[m \u001b[34mRTX A6000       \u001b[m |\u001b[1m\u001b[31m 82'C\u001b[m, \u001b[1m\u001b[32m 95 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m45208\u001b[m / \u001b[33m48685\u001b[m MB | \u001b[1m\u001b[30mgkissas\u001b[m(\u001b[33m45205M\u001b[m)\r\n",
      "\u001b[36m[4]\u001b[m \u001b[34mRTX A6000       \u001b[m |\u001b[1m\u001b[31m 50'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    1\u001b[m / \u001b[33m48685\u001b[m MB |\r\n",
      "\u001b[36m[5]\u001b[m \u001b[34mRTX A6000       \u001b[m |\u001b[31m 31'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    1\u001b[m / \u001b[33m48685\u001b[m MB |\r\n",
      "\u001b[36m[6]\u001b[m \u001b[34mRTX A6000       \u001b[m |\u001b[31m 33'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    1\u001b[m / \u001b[33m48685\u001b[m MB |\r\n",
      "\u001b[36m[7]\u001b[m \u001b[34mRTX A6000       \u001b[m |\u001b[31m 31'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m    1\u001b[m / \u001b[33m48685\u001b[m MB |\r\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "from numpy import fft\n",
    "import jax.numpy as np\n",
    "from jax import random, grad, vmap, jit, lax\n",
    "from jax.experimental import optimizers\n",
    "from jax.experimental.optimizers import make_schedule\n",
    "from jax.experimental.ode import odeint\n",
    "from jax.nn import relu, leaky_relu, swish, sigmoid\n",
    "from jax.config import config\n",
    "\n",
    "import itertools\n",
    "from functools import partial\n",
    "from torch.utils import data\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "from jax import jacobian, eval_shape\n",
    "from jax.tree_util import tree_map, tree_multimap, tree_reduce \n",
    "from jax.flatten_util import ravel_pytree\n",
    "from jax.ops import index_update, index\n",
    "\n",
    "from jaxpinns.optimizers import mdmm_adam\n",
    "\n",
    "import neural_tangents as nt\n",
    "import operator\n",
    "\n",
    "from jax.scipy.stats.norm import logpdf, pdf\n",
    "\n",
    "\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "import time\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = ''\n",
    "save_path = 'regularized/color/workshop_final/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rc('font', family='serif')\n",
    "plt.rcParams.update({'font.size': 16,\n",
    "                        'lines.linewidth': 2,\n",
    "                        'axes.labelsize': 10,\n",
    "                        'axes.titlesize': 16,\n",
    "                        'xtick.labelsize': 10,\n",
    "                        'ytick.labelsize': 10,\n",
    "                        'legend.fontsize': 10,\n",
    "                        'axes.linewidth': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(\"../helpers.py\").read())\n",
    "exec(open(\"../regularized_image_regression_helpers.py\").read())\n",
    "\n",
    "\n",
    "\n",
    "def d1_regularization(f):\n",
    "    def pde(params, x):\n",
    "        f_r = lambda params, x: f(params, x)[0]\n",
    "        f_g = lambda params, x: f(params, x)[1]\n",
    "        f_b = lambda params, x: f(params, x)[2]\n",
    "        return np.array([grad(f_r, argnums = 1)(params, x), grad(f_g, argnums = 1)(params, x),\\\n",
    "                         grad(f_b, argnums = 1)(params, x)])\n",
    "    return pde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(907, 907, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], np.array([0.2989, 0.5870, 0.1140])).reshape(rgb.shape[:2] + (1,))\n",
    "file_name = 'ucd.jpg'\n",
    "img = image.imread(source_path + file_name)\n",
    "# Y_raw = np.array(rgb2gray(img))\n",
    "Y_raw = np.array(img)\n",
    "Y_raw = Y_raw[::3][:,::3]\n",
    "Y_raw = Y_raw[:min(Y_raw.shape[:2])][:,:min(Y_raw.shape[:2])]\n",
    "Y_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw = np.array(np.meshgrid(np.arange(Y_raw.shape[0]), np.arange(Y_raw.shape[1]))).reshape(2,-1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of pixel used: 0.99\n"
     ]
    }
   ],
   "source": [
    "# ratio = 0.99\n",
    "train_idx = random.choice(key, np.arange(X_raw.shape[0]), shape = (int(X_raw.shape[0] * ratio), ), replace = False)\n",
    "\n",
    "X_train = X_raw[train_idx]\n",
    "mu_X, sigma_X = X_train.mean(0), X_train.std(0)\n",
    "\n",
    "Y_train = Y_raw.reshape(-1,Y_raw.shape[-1])[train_idx]\n",
    "mu_Y, sigma_Y = Y_train.mean(0), Y_train.std(0)\n",
    "Y_train = (Y_train - mu_Y) / sigma_Y\n",
    "\n",
    "print(f'Ratio of pixel used: {ratio}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(Y_raw.shape).reshape(-1,Y_raw.shape[-1])\n",
    "mask = index_update(mask, train_idx, 1).reshape(Y_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "# batch_size = 2048\n",
    "rcs_sampler = residual_sampler(batch_size, Y_raw.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colored\n",
    "width = 512\n",
    "depth = 8\n",
    "\n",
    "layers = [2, *[width for _ in range(depth)], Y_raw.shape[-1]]\n",
    "\n",
    "# Pure sine function, not scaled sine\n",
    "activation = np.sin\n",
    "save_path = save_path + 'SIREN_'\n",
    "\n",
    "dist = lambda key, shape: random.uniform(key, shape, minval = -np.sqrt(6), maxval = np.sqrt(6))\n",
    "init_model = normInitNet(layers[:-1], activation, mu_X, sigma_X, dist = dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important: confirm lam 0.0001 before running!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "init_fn, update_fn, get_params = optimizers.adam(lr)\n",
    "# init_fn, update_fn, get_params = optimizers.nesterov(lr, 0.9)\n",
    "lam = 1e-4\n",
    "@jit\n",
    "def step(i, state, X):\n",
    "    key = random.PRNGKey(i)\n",
    "    g = grad(init_model.regulated_logSineLoss)(get_params(state), init_model.net_init(key), X, lam = lam)\n",
    "    return update_fn(i, g, state)\n",
    "print(f'Important: confirm lam {lam} before running!!!!!!!!!!!!!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:49<00:00, 406.53it/s, Log sine loss=0.0021783365]\n"
     ]
    }
   ],
   "source": [
    "opt_state = init_fn(init_model.scale_params)\n",
    "init_nIter = 20000\n",
    "pbar = trange(init_nIter)\n",
    "scale_stor = []\n",
    "bias_stor = []\n",
    "\n",
    "for i in pbar:\n",
    "    mini_batch = rcs_sampler.sample(random.PRNGKey(i))\n",
    "    opt_state = step(i, opt_state, mini_batch)\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        params = get_params(opt_state)\n",
    "        pbar.set_postfix({'Log sine loss': init_model.logSineLoss(get_params(opt_state), init_model.net_init(random.PRNGKey(i)), mini_batch)})\n",
    "        scale_stor.append([w for (w, b) in params])\n",
    "        bias_stor.append([b for (w, b) in params])\n",
    "        \n",
    "\n",
    "scale_stor = np.array(scale_stor)\n",
    "bias_stor = np.array(bias_stor)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bcs_sampler = boundary_sampler(batch_size, X_train, Y_train)\n",
    "fit_model = fitPinns(layers, activation, mu_X, sigma_X, d1_regularization, (), dist = dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model.scaled_net_params = parameter_scaling(fit_model.net_params[:-1], scale_params) + [fit_model.net_params[-1]]\n",
    "original_scale_parameters = [[1., 0.] for _ in scale_params]\n",
    "original_scale_parameters[0][0] = 30.\n",
    "fit_model.orginal_scaled_net_params = parameter_scaling(fit_model.net_params[:-1], original_scale_parameters) + [fit_model.net_params[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 1e-3\n",
    "# init_fn, update_fn, get_params = optimizers.nesterov(lr, 0.9)\n",
    "\n",
    "init_fn, update_fn, get_params = optimizers.adam(optimizers.exponential_decay(1e-4, decay_steps = 1000, decay_rate = 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nIter = 400000\n",
    "losses = []\n",
    "regularizer = 1e-2\n",
    "more_regularizer = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [23:52<00:00, 279.16it/s, boundary loss=8.18e-03 8.57e-03, residual loss=4.46e-02 3.99e-02]\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def step(i, state, batch):\n",
    "    g1 = grad(fit_model.boundary_loss)(get_params(state), batch[0])\n",
    "\n",
    "    return update_fn(i, g1, state)\n",
    "\n",
    "pbar = trange(nIter)\n",
    "log = 1000\n",
    "# stack the parameter trees at each leaf node for vmap\n",
    "init_paramses = tree_multimap(stack_fn, fit_model.net_params,\n",
    "                              fit_model.scaled_net_params,\n",
    "                              fit_model.orginal_scaled_net_params)\n",
    "opt_state = vmap(init_fn)(init_paramses)\n",
    "# compile the vmap optimization step\n",
    "v_step = jit(vmap(step, in_axes = (None, 0, None)))\n",
    "loss_stor = []\n",
    "\n",
    "rcs_place_holder = rcs_sampler.sample(key)\n",
    "for i in pbar:\n",
    "    key,_ = random.split(key)\n",
    "    mini_batch = (bcs_sampler.sample(key), rcs_place_holder)\n",
    "    opt_state = v_step(i, opt_state, mini_batch)\n",
    "    if i % log == 0:\n",
    "        params = vmap(get_params)(opt_state)\n",
    "\n",
    "        bcs_loss = vmap(fit_model.boundary_loss, in_axes = [0, None])(params, mini_batch[0])\n",
    "        rcs_loss = vmap(fit_model.residual_loss, in_axes = [0, None])(params, mini_batch[1])\n",
    "        loss_stor.append((bcs_loss, rcs_loss))\n",
    "        \n",
    "        pbar.set_postfix({'boundary loss': f'{bcs_loss[0]:.2e} {bcs_loss[1]:.2e} {bcs_loss[2]:.2e}',\\\n",
    "                         'residual loss': f'{rcs_loss[0]:.2e} {rcs_loss[1]:.2e}, {rcs_loss[2]:.2e}'})\n",
    "losses.append(loss_stor) \n",
    "        \n",
    "\n",
    "opt_params = vmap(get_params)(opt_state)\n",
    "normal_opt_params = tree_map(lambda x: x[0], opt_params)\n",
    "scaled_opt_params = tree_map(lambda x: x[1], opt_params)\n",
    "original_opt_params = tree_map(lambda x: x[2], opt_params)\n",
    "\n",
    "normal_pred_img = (fit_model.net_apply(normal_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n",
    "scaled_pred_img = (fit_model.net_apply(scaled_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n",
    "original_pred_img = (fit_model.net_apply(original_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [58:55<00:00, 113.14it/s, boundary loss=8.03e-03 8.67e-03, residual loss=3.72e-02 3.48e-02] \n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def step(i, state, batch, w):\n",
    "    g1 = grad(fit_model.boundary_loss)(get_params(state), batch[0])\n",
    "    g2 = grad(fit_model.residual_loss)(get_params(state), batch[1])\n",
    "    g = tree_multimap(lambda x,y: w[0] * x + y * w[1], g1, g2)\n",
    "    return update_fn(i, g, state)\n",
    "\n",
    "pbar = trange(nIter)\n",
    "# stack the parameter trees at each leaf node for vmap\n",
    "init_paramses = tree_multimap(stack_fn, fit_model.net_params,\n",
    "                              fit_model.scaled_net_params,\n",
    "                              fit_model.orginal_scaled_net_params)\n",
    "opt_state = vmap(init_fn)(init_paramses)\n",
    "# compile the vmap optimization step\n",
    "v_step = jit(vmap(step, in_axes = (None, 0, None, None)))\n",
    "loss_stor = []\n",
    "weight = [1., regularizer]\n",
    "for i in pbar:\n",
    "    key,_ = random.split(key)\n",
    "    mini_batch = (bcs_sampler.sample(key), rcs_sampler.sample(key))\n",
    "    opt_state = v_step(i, opt_state, mini_batch, weight)\n",
    "    if i % log == 0:\n",
    "        params = vmap(get_params)(opt_state)\n",
    "\n",
    "        bcs_loss = vmap(fit_model.boundary_loss, in_axes = [0, None])(params, mini_batch[0])\n",
    "        rcs_loss = vmap(fit_model.residual_loss, in_axes = [0, None])(params, mini_batch[1])\n",
    "        loss_stor.append((bcs_loss, rcs_loss))\n",
    "        \n",
    "        pbar.set_postfix({'boundary loss': f'{bcs_loss[0]:.2e} {bcs_loss[1]:.2e} {bcs_loss[2]:.2e}',\\\n",
    "                         'residual loss': f'{rcs_loss[0]:.2e} {rcs_loss[1]:.2e}, {rcs_loss[2]:.2e}'})\n",
    "losses.append(loss_stor) \n",
    "        \n",
    "\n",
    "opt_params = vmap(get_params)(opt_state)\n",
    "regularized_normal_opt_params = tree_map(lambda x: x[0], opt_params)\n",
    "regularized_scaled_opt_params = tree_map(lambda x: x[1], opt_params)\n",
    "regularized_original_opt_params = tree_map(lambda x: x[2], opt_params)\n",
    "\n",
    "regularized_normal_pred_img = (fit_model.net_apply(regularized_normal_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n",
    "regularized_scaled_pred_img = (fit_model.net_apply(regularized_scaled_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n",
    "regularized_original_pred_img = (fit_model.net_apply(regularized_original_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400000/400000 [58:39<00:00, 113.66it/s, boundary loss=1.57e-02 1.45e-02, residual loss=1.29e-02 1.26e-02] \n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def step(i, state, batch, w):\n",
    "    g1 = grad(fit_model.boundary_loss)(get_params(state), batch[0])\n",
    "    g2 = grad(fit_model.residual_loss)(get_params(state), batch[1])\n",
    "    g = tree_multimap(lambda x,y: w[0] * x + y * w[1], g1, g2)\n",
    "    return update_fn(i, g, state)\n",
    "\n",
    "pbar = trange(nIter)\n",
    "# stack the parameter trees at each leaf node for vmap\n",
    "init_paramses = tree_multimap(stack_fn, fit_model.net_params,\n",
    "                              fit_model.scaled_net_params,\n",
    "                              fit_model.orginal_scaled_net_params)\n",
    "opt_state = vmap(init_fn)(init_paramses)\n",
    "# compile the vmap optimization step\n",
    "v_step = jit(vmap(step, in_axes = (None, 0, None, None)))\n",
    "loss_stor = []\n",
    "weight = [1., more_regularizer]\n",
    "for i in pbar:\n",
    "    key,_ = random.split(key)\n",
    "    mini_batch = (bcs_sampler.sample(key), rcs_sampler.sample(key))\n",
    "    opt_state = v_step(i, opt_state, mini_batch, weight)\n",
    "    if i % log == 0:\n",
    "        params = vmap(get_params)(opt_state)\n",
    "\n",
    "        bcs_loss = vmap(fit_model.boundary_loss, in_axes = [0, None])(params, mini_batch[0])\n",
    "        rcs_loss = vmap(fit_model.residual_loss, in_axes = [0, None])(params, mini_batch[1])\n",
    "        loss_stor.append((bcs_loss, rcs_loss))\n",
    "        \n",
    "        pbar.set_postfix({'boundary loss': f'{bcs_loss[0]:.2e} {bcs_loss[1]:.2e} {bcs_loss[2]:.2e}',\\\n",
    "                         'residual loss': f'{rcs_loss[0]:.2e} {rcs_loss[1]:.2e}, {rcs_loss[2]:.2e}'})\n",
    "losses.append(loss_stor) \n",
    "        \n",
    "\n",
    "opt_params = vmap(get_params)(opt_state)\n",
    "more_regularized_normal_opt_params = tree_map(lambda x: x[0], opt_params)\n",
    "more_regularized_scaled_opt_params = tree_map(lambda x: x[1], opt_params)\n",
    "more_regularized_original_opt_params = tree_map(lambda x: x[2], opt_params)\n",
    "\n",
    "more_regularized_normal_pred_img = (fit_model.net_apply(more_regularized_normal_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n",
    "more_regularized_scaled_pred_img = (fit_model.net_apply(more_regularized_scaled_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n",
    "more_regularized_original_pred_img = (fit_model.net_apply(more_regularized_original_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.array(losses)\n",
    "np.save(save_path + file_name[:-4] + \\\n",
    "        f'_width_{width}_depth_{depth}_multi_regularized_{batch_size}_{ratio}_losses',\\\n",
    "        losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(save_path + file_name[:-4] + \\\n",
    "        f'_width_{width}_depth_{depth}_multi_regularized_{batch_size}_{ratio}_flat_params',\\\n",
    "        np.array((ravel_pytree(normal_opt_params)[0],\n",
    "                  ravel_pytree(scaled_opt_params)[0],\n",
    "                  ravel_pytree(original_opt_params)[0],\n",
    "                  ravel_pytree(regularized_normal_opt_params)[0],\n",
    "                  ravel_pytree(regularized_scaled_opt_params)[0],\n",
    "                  ravel_pytree(regularized_original_opt_params)[0],\n",
    "                  ravel_pytree(more_regularized_normal_opt_params)[0],\n",
    "                  ravel_pytree(more_regularized_scaled_opt_params)[0],\n",
    "                  ravel_pytree(more_regularized_original_opt_params)[0]))\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_params = np.load(save_path + file_name[:-4] + \\\n",
    "        f'_width_{width}_depth_{depth}_multi_regularized_{batch_size}_{ratio}_flat_params.npy')\n",
    "_, unravel = ravel_pytree(fit_model.scaled_net_params)\n",
    "\n",
    "normal_opt_params = unravel(flat_params[0])\n",
    "scaled_opt_params = unravel(flat_params[1])\n",
    "original_opt_params = unravel(flat_params[2])\n",
    "regularized_normal_opt_params = unravel(flat_params[3])\n",
    "regularized_scaled_opt_params = unravel(flat_params[4])\n",
    "regularized_original_opt_params = unravel(flat_params[5])\n",
    "more_regularized_normal_opt_params = unravel(flat_params[6])\n",
    "more_regularized_scaled_opt_params = unravel(flat_params[7])\n",
    "more_regularized_original_opt_params = unravel(flat_params[8])\n",
    "\n",
    "normal_pred_img = (fit_model.net_apply(normal_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n",
    "scaled_pred_img = (fit_model.net_apply(scaled_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n",
    "original_pred_img = (fit_model.net_apply(original_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n",
    "\n",
    "regularized_normal_pred_img = (fit_model.net_apply(regularized_normal_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n",
    "regularized_scaled_pred_img = (fit_model.net_apply(regularized_scaled_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n",
    "regularized_original_pred_img = (fit_model.net_apply(regularized_original_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n",
    "\n",
    "more_regularized_normal_pred_img = (fit_model.net_apply(more_regularized_normal_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n",
    "more_regularized_scaled_pred_img = (fit_model.net_apply(more_regularized_scaled_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n",
    "more_regularized_original_pred_img = (fit_model.net_apply(more_regularized_original_opt_params, X_raw) * sigma_Y + mu_Y).reshape(Y_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR of Xavier initialization: 25.077484130859375, 24.991300582885742, 21.95233154296875\n",
      "SNR of scaled initialization: 25.155033111572266, 25.05310821533203, 22.035398483276367\n"
     ]
    }
   ],
   "source": [
    "SNR = lambda pred, y: -np.log(np.linalg.norm(pred - y) / np.linalg.norm(y)) / np.log(10) * 10 * 2\n",
    "\n",
    "print(f'SNR of Xavier initialization: {SNR(normal_pred_img, Y_raw)}, {SNR(regularized_normal_pred_img, Y_raw)}, {SNR(more_regularized_normal_pred_img, Y_raw)}')\n",
    "print(f'SNR of scaled initialization: {SNR(scaled_pred_img, Y_raw)}, {SNR(regularized_scaled_pred_img, Y_raw)}, {SNR(more_regularized_scaled_pred_img, Y_raw)}')\n",
    "print(f'SNR of original initialization: {SNR(original_pred_img, Y_raw)}, {SNR(regularized_original_pred_img, Y_raw)}, {SNR(more_original_pred_img, Y_raw)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path + file_name[:-4] + \\\n",
    "          f'_width_{width}_depth_{depth}_multi_regularized_{batch_size}_{ratio}_SNR.txt', \"w\") as text_file:\n",
    "    text_file.write(f'regularization: 0, {regularizer}, {more_regularizer}\\n')\n",
    "    text_file.write(f'SNR of Xavier initialization: {SNR(normal_pred_img, Y_raw)}, {SNR(regularized_normal_pred_img, Y_raw)}, {SNR(more_regularized_normal_pred_img, Y_raw)}\\n')\n",
    "    text_file.write(f'SNR of scaled initialization: {SNR(scaled_pred_img, Y_raw)}, {SNR(regularized_scaled_pred_img, Y_raw)}, {SNR(more_regularized_scaled_pred_img, Y_raw)}\\n')\n",
    "    text_file.write(f'SNR of original initialization: {SNR(original_pred_img, Y_raw)}, {SNR(regularized_original_pred_img, Y_raw)}, {SNR(more_regularized_original_pred_img, Y_raw)}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
